{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xu2SVpFJjmJr"
   },
   "source": [
    "# DeepDreaming with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "hupz2hrZjdnC"
   },
   "source": [
    ">[Loading and displaying the model graph](#loading)\n",
    "\n",
    ">[Classifying Images](#classify)\n",
    "\n",
    ">[Naive feature visualization](#naive)\n",
    "\n",
    ">[Multiscale image generation](#multiscale)\n",
    "\n",
    ">[Laplacian Pyramid Gradient Normalization](#laplacian)\n",
    "\n",
    ">[Playing with feature visualzations](#playing)\n",
    "\n",
    ">[DeepDream](#deepdream)\n",
    "\n",
    ">[More Fun!](#fun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PLC9SvcQgkG"
   },
   "source": [
    "# This tutorial will show how to work with TensorFlow, and how to use image classification models. \n",
    "You will benefit most from this if you have some working knowledge of python, and if you have a rough idea what a neural network is and how it works. \n",
    "\n",
    "You will learn about deep neural networks, in particular convolutional neural networks, and how they are used for image classification tasks. We will also gain \n",
    "an intuitive understanding how neural network represent information they have learned.\n",
    "\n",
    "Specifically, you will learn how to:\n",
    "\n",
    "- Load and a pre-trained TensorFlow model and inspect it.\n",
    "- Classify images using TensorFlow using a pre-trained model.\n",
    "- Visualize feature channels from a convolutional network to understand what it has learned.\n",
    "- Enhance what the network sees in a given image to produce dream-like images.\n",
    "\n",
    "The network we'll examine is [Inception-v3](http://arxiv.org/abs/1512.00567). It's trained to classify an image into 1 of the 1000 categories from the [ImageNet](http://image-net.org/) dataset. For a good introduction to neural networks, see the book [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen. For background on convolutional networks, see Chris Olah's excellent [blog post](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/).\n",
    "\n",
    "As discussed in [Inceptionism: Going Deeper into Neural Networks](http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html), our goal is to visualize the internal image representations learned by a network trained to classify images. We'll make these visualizations both efficient to generate, and even beautiful.\n",
    "\n",
    "Impatient readers can start with exploring the full galleries of images generated by the method described here for [GoogLeNet](http://storage.googleapis.com/deepdream/visualz/tensorflow_inception/index.html) and [VGG16](http://storage.googleapis.com/deepdream/visualz/vgg16/index.html) architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": []
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1457963606294,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "jtD9nb-2QgkY",
    "outputId": "b935629b-8608-45c1-942f-612b7dbb13d3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# boilerplate code\n",
    "import os\n",
    "import re\n",
    "from cStringIO import StringIO\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you are using getting an import error with PIL, you will need to install Pillow, via:*\n",
    "\n",
    "```\n",
    "sudo pip install Pillow\n",
    "```\n",
    "\n",
    "*This is required of the current Docker image as well.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILvNKvMvc2n5"
   },
   "source": [
    "<a id='loading'></a>\n",
    "## Loading and displaying the model graph\n",
    "\n",
    "The pretrained network can be downloaded [here](http://download.tensorflow.org/models/inception5h.zip). If it is not already here, download it and unpack the archive. The actual network is stored in the file `tensorflow_inception_graph.pb`. Set the `model_fn` variable to its path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the network and prepare it for input. TensorFlow maintains a computation graph and a session, which maintains state for running computations and which can be executed remotely. We will first make a fresh graph and a session which uses that graph. The session will be used in the rest of the tutorial.\n",
    "\n",
    "We then load the model. The model consists of a computation graph which happens to have a node called \"input\", into which we need to feed a batch of input images. The input node in the graph expects images that are normalized by subtracting the average brightness of all images in the imagenet dataset. \n",
    "\n",
    "Because we will use single, unnormalized images, we will make a small graph that takes an image, subtracts the imagenet mean, and expands it to look like a batch of images. \n",
    "\n",
    "We then load the graph from file and import it into the default graph for our session. The little importer graph is now connected to the input node in the loaded graph, and we can feed regular images into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": []
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 2264,
     "status": "ok",
     "timestamp": 1457962713799,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "761b412462cda2d0",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "1kJuJRLiQgkg",
    "outputId": "d2aaf8cc-91e1-4864-8cf8-0aef612db1d6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating fresh Graph and TensorFlow session\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "# Prepare input for the format expected by the graph\n",
    "t_input = tf.placeholder(np.float32, name='our_input') # define the input tensor\n",
    "imagenet_mean = 117.0\n",
    "t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n",
    "\n",
    "# Load graph and import into graph used by our session\n",
    "model_fn = 'imagenet/tensorflow_inception_graph.pb'\n",
    "graph_def = tf.GraphDef.FromString(open(model_fn).read())\n",
    "tf.import_graph_def(graph_def, {'input':t_preprocessed})\n",
    "\n",
    "print \"imported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first count how many layers there are in this graph (we'll only count the convolutional layers), and how many total features this graph uses internally. We'll look at what those features look like later, we have enough to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]\n",
    "feature_nums = [int(graph.get_tensor_by_name(name+':0').get_shape()[-1]) for name in layers]\n",
    "\n",
    "print 'Number of layers', len(layers)\n",
    "print 'Total number of feature channels:', sum(feature_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look at what the graph looks like. We use tensorboard to visualize the graph, first stripping large constants (containing the pre-trained network weights) to speed things up. We can use the names shown in the diagram to identify layers we'd like to look into. Be sure to expand the \"mixed\" node, which contains the bulk of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1198,
     "status": "ok",
     "timestamp": 1457962715078,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "761b412462cda2d0",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "LrucdvgyQgks",
    "outputId": "5936270b-5da8-4825-b2e9-145c494d36e6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Helper functions for TF Graph visualization\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "  \n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "  \n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "# Visualizing the network graph. Be sure expand the \"mixed\" nodes to see their \n",
    "# internal structure. We are going to visualize \"Conv2D\" nodes.\n",
    "tmp_def = rename_nodes(graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJZVMSmiQgkp"
   },
   "source": [
    "To take a glimpse into the kinds of patterns that the network learned to recognize, we will try to generate images that maximize the sum of activations of particular channel of a particular convolutional layer of the neural network. The network we explore contains many convolutional layers, each of which outputs tens to hundreds of feature channels, so we have plenty of patterns to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classify'></a>\n",
    "## Classifying Images\n",
    "\n",
    "Let's first classify some images using this graph. The softmax layer contains the network's predictions in form of numerical IDs. To translate those to human-readable values, we have to load some translation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_lookup_path = 'imagenet/imagenet_2012_challenge_label_map_proto.pbtxt'\n",
    "uid_lookup_path = 'imagenet/imagenet_synset_to_human_label_map.txt'\n",
    "\n",
    "# The id translation goes via id strings -- find translation between UID string and human-friendly names\n",
    "proto_as_ascii_lines = open(uid_lookup_path).readlines()\n",
    "uid_to_human = {}\n",
    "p = re.compile(r'[n\\d]*[ \\S,]*')\n",
    "for line in proto_as_ascii_lines:\n",
    "    parsed_items = p.findall(line)\n",
    "    uid = parsed_items[0]\n",
    "    human_string = parsed_items[2]\n",
    "    uid_to_human[uid] = human_string\n",
    "\n",
    "# Get node IDs to UID strings map\n",
    "proto_as_ascii_lines = open(label_lookup_path).readlines()\n",
    "node_id_to_uid = {}\n",
    "for line in proto_as_ascii_lines:\n",
    "    if line.startswith('  target_class:'):\n",
    "        target_class = int(line.split(': ')[1])\n",
    "    if line.startswith('  target_class_string:'):\n",
    "        target_class_string = line.split(': ')[1]\n",
    "        node_id_to_uid[target_class] = target_class_string[1:-2]\n",
    "\n",
    "# Make node ID to human friendly names map\n",
    "node_id_to_name = {}\n",
    "for key, val in node_id_to_uid.iteritems():\n",
    "    name = uid_to_human[val]\n",
    "    node_id_to_name[key] = name\n",
    "    \n",
    "# make sure we have a name for each possible ID\n",
    "for i in range(graph.get_tensor_by_name('import/softmax2:0').get_shape()[1]):\n",
    "    if i not in node_id_to_name:\n",
    "        node_id_to_name[i] = '???'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can find out what any neuron stands for. Imagenet has 1000 different classes, some of them pretty obscure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_id_to_name[438]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the network are contained in the output of the softmax layer. In the network we loaded, the relevant layer is called \"softmax2\". We'll make a small function which feeds an input image, reads the result from the graph and translates it, plus an additional function that will create an image from an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to get a named layer from the graph\n",
    "def T(layer_name):\n",
    "    return graph.get_tensor_by_name(\"import/%s:0\" % layer_name)\n",
    "\n",
    "softmax = T('softmax2')\n",
    "\n",
    "def prep_img(filename):\n",
    "    size = (224, 224)\n",
    "    img = PIL.Image.open(filename)\n",
    "    img.thumbnail(size, PIL.Image.ANTIALIAS)\n",
    "    thumb = PIL.ImageOps.fit(img, size, PIL.Image.ANTIALIAS, (0.5, 0.5))\n",
    "    return np.float32(thumb)\n",
    "\n",
    "# Print the 5 top predictions for a given image\n",
    "def prediction(filename, k=5):\n",
    "    img = prep_img(filename)\n",
    "    # Load, resize, and central square crop the image.\n",
    "    \n",
    "    # Compute predictions\n",
    "    predictions = sess.run(softmax, {t_input: img})\n",
    "    predictions = np.squeeze(predictions)\n",
    "\n",
    "    top_k = predictions.argsort()[-k:][::-1]\n",
    "    for node_id in top_k:\n",
    "        human_string = node_id_to_name[node_id]\n",
    "        score = predictions[node_id]\n",
    "        print '%s (score = %.5f)' % (human_string, score)\n",
    "        \n",
    "# Helper function: Display an image\n",
    "def showarray(a, fmt='jpeg', size=None):\n",
    "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
    "    f = StringIO()\n",
    "    img = PIL.Image.fromarray(a)\n",
    "    if size is not None:\n",
    "        img = img.resize((size,size))\n",
    "    img.save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "        \n",
    "print \"defined\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can classify a panda. You should also try other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showarray(prep_img('testimages/deer.jpg')/255., size=500)\n",
    "prediction('testimages/deer.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nv2JqNLBhy1j"
   },
   "source": [
    "<a id='naive'></a>\n",
    "## Naive feature visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LXaGEJkQgk4"
   },
   "source": [
    "This tells us what the network thinks is contained in an image, but we still don't know why. So let's start to look at the features the network has learned to recognize. \n",
    "\n",
    "First, we'll define some helper functions to show images, and an input image with a bit of random noise, which is always useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize an image for visualization\n",
    "def visstd(a, s=0.1):\n",
    "    return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n",
    "\n",
    "img_noise = np.random.uniform(size=(224,224,3)) + 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer operates directly on the image. It contains a weight tensor taking a 7x7 patch of the image and computing 64 features from it. We can look at the learned weights in this layer to see what kinds of features these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to see all the possible weights\n",
    "print '\\n'.join([op.name[7:] for op in graph.get_operations() if op.name.endswith('w')])\n",
    "w = T('conv2d0_w')\n",
    "shape = w.get_shape().as_list()\n",
    "print shape\n",
    "feature_id = 14\n",
    "weights = tf.squeeze(tf.slice(w, [0, 0, 0, feature_id], [-1, -1, -1, 1])).eval()\n",
    "showarray(visstd(weights), size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're trained to look at filter kernels, this may be useful to you. You can also look at what the output of the convolution looks like, by running the first layer on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = T('conv2d0')\n",
    "feature = tf.squeeze(tf.slice(a, [0, 0, 0, feature_id], [-1, -1, -1, 1]))\n",
    "img = prep_img('testimages/deer.jpg')\n",
    "feature = sess.run(feature, {t_input: img})\n",
    "showarray(visstd(feature), size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in the first layer, the weights are not easy to interpret. For all layers except the first, it is almost impossible. We have to find another way of understanding what the network has learned. \n",
    "\n",
    "We use a simple visualization technique to show what any given feature looks like: Image space gradient ascent. This works as follows: We pick a feature plane from any layer in the network. This feature plane recognizes the presence of a specific feature in the image. We will try to generate an image that maximizes this feature signal. We start with an image that is just noise, and compute the gradient of the feature signal (averaged over the whole image) with respect to the input image. We then modify the input image to increase the feature signal. This generates an image that this specific network layer thinks is full of whatever feature it is meant to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_naive(t_obj, img0=img_noise, iter_n=20, step=1.0):\n",
    "    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
    "    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
    "    \n",
    "    img = img0.copy()\n",
    "    for i in xrange(iter_n):\n",
    "        g, score = sess.run([t_grad, t_score], {t_input:img})\n",
    "        # normalizing the gradient, so the same step size should work \n",
    "        g /= g.std()+1e-8         # for different layers and networks\n",
    "        img += g*step\n",
    "        print score,\n",
    "    clear_output()\n",
    "    showarray(visstd(img), size=448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to pick a layer and a feature channel to visualize. The following cell will enumerate all the available layers.  Refer to the graph above to see where they are. \n",
    "\n",
    "Earlier layers (closer to the bottom, i.e. the input) have lower level features, later layers have higher level features. Note that we use layer outputs before applying the ReLU nonlinearity in order to have non-zero gradients for features with negative initial activations (hence the \"pre_relu\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell if you'd like a list of all layers to pick from\n",
    "print '\\n'.join([op.name[7:] for op in graph.get_operations() if op.name.endswith('pre_relu')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering\n",
    "\n",
    "At this point, we can render one of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1457962479327,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "761b412462cda2d0",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "ZxC_XGGXQgk7",
    "outputId": "1c971a74-bf65-4069-cfd0-1473aa909a83",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pick any internal layer. \n",
    "layer = 'mixed4d_3x3_bottleneck_pre_relu'\n",
    "print '%d channels in layer.' % T(layer).get_shape()[-1] \n",
    "\n",
    "# Pick a feature channel to visualize\n",
    "channel = 139\n",
    "\n",
    "render_naive(T(layer)[:,:,:,channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZroBKE5YiDsb"
   },
   "source": [
    "<a id=\"multiscale\"></a>\n",
    "## Multiscale image generation\n",
    "\n",
    "Looks like the network wants to show us something interesting! Let's help it. We are going to apply gradient ascent on multiple scales. Details formed on smaller scale will be upscaled and augmented with additional details on the next scale. \n",
    "\n",
    "Basically, instead of starting from a random noise image, we start only the first iteration (octave) from random noise, and each octave after we start from the upsampled result of the previous optimization on the smaller image.\n",
    "\n",
    "With multiscale image generation it may be tempting to set the number of octaves to some high value to produce wallpaper-sized images. Storing network activations and backprop values will quickly run out of GPU memory in this case. There is a simple trick to avoid this: split the image into smaller tiles and compute each tile gradient independently. Applying random shifts to the image before every iteration helps avoid tile seams and improves the overall image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1457963844162,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "2iwWSOgsQglG",
    "outputId": "221dae81-914b-4167-eb49-26ef2d431a66",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tffunc(*argtypes):\n",
    "    '''Helper that transforms TF-graph generating function into a regular one.\n",
    "    See \"resize\" function below.\n",
    "    '''\n",
    "    placeholders = map(tf.placeholder, argtypes)\n",
    "    def wrap(f):\n",
    "        out = f(*placeholders)\n",
    "        def wrapper(*args, **kw):\n",
    "            return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))\n",
    "        return wrapper\n",
    "    return wrap\n",
    "\n",
    "# Helper function that uses TF to resize an image\n",
    "def resize(img, size):\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    return tf.image.resize_bilinear(img, size)[0,:,:,:]\n",
    "resize = tffunc(np.float32, np.int32)(resize)\n",
    "\n",
    "def calc_grad_tiled(img, t_grad, tile_size=512):\n",
    "    '''Compute the value of tensor t_grad over the image in a tiled way.\n",
    "    Random shifts are applied to the image to blur tile boundaries over \n",
    "    multiple iterations.'''\n",
    "    sz = tile_size\n",
    "    h, w = img.shape[:2]\n",
    "    sx, sy = np.random.randint(sz, size=2)\n",
    "    img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n",
    "    grad = np.zeros_like(img)\n",
    "    for y in xrange(0, max(h-sz//2, sz),sz):\n",
    "        for x in xrange(0, max(w-sz//2, sz),sz):\n",
    "            sub = img_shift[y:y+sz,x:x+sz]\n",
    "            g = sess.run(t_grad, {t_input:sub})\n",
    "            grad[y:y+sz,x:x+sz] = g\n",
    "    return np.roll(np.roll(grad, -sx, 1), -sy, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1457963487829,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "GRCJdG8gQglN",
    "outputId": "7e21352d-9131-4f81-a52f-912b2e299475",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def render_multiscale(t_obj, img0=img_noise, iter_n=10, step=1.0, octave_n=2, octave_scale=1.4):\n",
    "    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
    "    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
    "    \n",
    "    img = img0.copy()\n",
    "    for octave in xrange(octave_n):\n",
    "        if octave > 0:\n",
    "            hw = np.float32(img.shape[:2])*octave_scale\n",
    "            img = resize(img, np.int32(hw))\n",
    "        for i in xrange(iter_n):\n",
    "            g = calc_grad_tiled(img, t_grad)\n",
    "            # normalizing the gradient, so the same step size should work \n",
    "            g /= g.std()+1e-8         # for different layers and networks\n",
    "            img += g*step\n",
    "            print '.',\n",
    "        clear_output()\n",
    "        showarray(visstd(img))\n",
    "        print 'Octave %d' % octave\n",
    "\n",
    "render_multiscale(T(layer)[:,:,:,channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDSZMtVYQglV"
   },
   "source": [
    "<a id=\"laplacian\"></a>\n",
    "## Laplacian Pyramid Gradient Normalization\n",
    "\n",
    "This looks better, but the resulting images mostly contain high frequencies. Can we improve it? One way is to add a smoothness prior into the optimization objective. This will effectively blur the image a little every iteration, suppressing the higher frequencies, so that the lower frequencies can catch up. This will require more iterations to produce a nice image. Why don't we just boost lower frequencies of the gradient instead? One way to achieve this is through the Laplacian pyramid decomposition. We call the resulting technique _Laplacian Pyramid Gradient Normalization_.\n",
    "\n",
    "We therefore split the image into its various frequency bands by repeatedly blurring it and extracting the highest frequencies by subtracting the blurred image (`lap_split` and `lap_split_n` below). We then normalize each frequency band separately (`normalize_std`), and then merge them back together by basically just adding them up (`lap_merge`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1457963876373,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "Do3WpFSUQglX",
    "outputId": "99835b80-ed6f-47a5-85c3-c77bd55d7b17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = np.float32([1,4,6,4,1])\n",
    "k = np.outer(k, k)\n",
    "k5x5 = k[:,:,None,None]/k.sum()*np.eye(3, dtype=np.float32)\n",
    "\n",
    "def lap_split(img):\n",
    "    '''Split the image into lo and hi frequency components'''\n",
    "    with tf.name_scope('split'):\n",
    "        lo = tf.nn.conv2d(img, k5x5, [1,2,2,1], 'SAME')  # Blurred image -- low frequencies only\n",
    "        lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(img), [1,2,2,1]) \n",
    "        hi = img-lo2 # hi is img with low frequencies removed\n",
    "    return lo, hi\n",
    "\n",
    "def lap_split_n(img, n):\n",
    "    '''Build Laplacian pyramid with n splits'''\n",
    "    levels = []\n",
    "    for i in xrange(n):\n",
    "        img, hi = lap_split(img)\n",
    "        levels.append(hi)\n",
    "    levels.append(img)\n",
    "    return levels[::-1] # List of images with lower and lower frequencies\n",
    "\n",
    "def lap_merge(levels):\n",
    "    '''Merge Laplacian pyramid'''\n",
    "    img = levels[0]\n",
    "    for hi in levels[1:]:\n",
    "        with tf.name_scope('merge'):\n",
    "            img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(hi), [1,2,2,1]) + hi\n",
    "    return img # Reconstructed image, all frequencies added back together\n",
    "\n",
    "def normalize_std(img, eps=1e-10):\n",
    "    '''Normalize image by making its standard deviation = 1.0'''\n",
    "    with tf.name_scope('normalize'):\n",
    "        std = tf.sqrt(tf.reduce_mean(tf.square(img)))\n",
    "        return img/tf.maximum(std, eps)\n",
    "\n",
    "def lap_normalize(img, scale_n=4):\n",
    "    '''Perform the Laplacian pyramid normalization.'''\n",
    "    img = tf.expand_dims(img,0)\n",
    "    tlevels = lap_split_n(img, scale_n) # Split into frequencies\n",
    "    tlevels = map(normalize_std, tlevels) # Normalize each frequency band\n",
    "    out = lap_merge(tlevels) # Put image back together\n",
    "    return out[0,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did all this in TensorFlow, so it generated a computation graph that we can inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Showing the lap_normalize graph with TensorBoard\n",
    "lap_graph = tf.Graph()\n",
    "with lap_graph.as_default():\n",
    "    lap_in = tf.placeholder(np.float32, name='lap_in')\n",
    "    lap_out = lap_normalize(lap_in)\n",
    "show_graph(lap_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now hav everything to render another image. The algorithm is unchanged, except that in each iteration, the gradient image is normalized using the Laplacian normalization graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 17273,
     "status": "ok",
     "timestamp": 1457964054088,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "zj8Ms-WqQgla",
    "outputId": "aa54c6c3-bf38-4054-f3f4-a5c82218e251",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def render_lapnorm(t_obj, img0=img_noise, visfunc=visstd,\n",
    "                   iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):\n",
    "    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
    "    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
    "    # Build the Laplacian normalization graph\n",
    "    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))\n",
    "\n",
    "    img = img0.copy()\n",
    "    for octave in xrange(octave_n):\n",
    "        if octave>0:\n",
    "            hw = np.float32(img.shape[:2])*octave_scale\n",
    "            img = resize(img, np.int32(hw))\n",
    "        for i in xrange(iter_n):\n",
    "            g = calc_grad_tiled(img, t_grad)\n",
    "            g = lap_norm_func(g)  # New!\n",
    "            img += g*step\n",
    "            print '.',\n",
    "        clear_output()\n",
    "        showarray(visfunc(img))\n",
    "\n",
    "render_lapnorm(T(layer)[:,:,:,channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzXJUF2lQgln"
   },
   "source": [
    "<a id=\"playing\"></a>\n",
    "## Playing with feature visualizations\n",
    "\n",
    "We got a nice smooth image using only 10 iterations per octave. In case of running on GPU this takes just a few seconds. Let's try to visualize another channel from the same layer. The network can generate wide diversity of patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 35857,
     "status": "ok",
     "timestamp": 1457964089937,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "a6jfiWqZQglq",
    "outputId": "40ebf3c4-a262-4e11-fca9-d42eb9306edb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_lapnorm(T(layer)[:,:,:,65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka6RyOMEnrB5"
   },
   "source": [
    "Lower layers produce features of lower complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1457967232229,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "KYOtrJxMnlws",
    "outputId": "8ec79dd8-259e-4bca-8115-705e76d1bc74",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_lapnorm(T('mixed3b_1x1_pre_relu')[:,:,:,101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuP8a4FlQglx"
   },
   "source": [
    "There are many interesting things one may try. For example, optimizing a linear combination of features gives a \"mixture\" pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 14408,
     "status": "ok",
     "timestamp": 1457964104157,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "ozN-nH2yQgl0",
    "outputId": "a890305e-7bed-4011-8535-5882d6b27482",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_lapnorm(T(layer)[:,:,:,65] + T(layer)[:,:,:,139], octave_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcPe-ZMv0dYR"
   },
   "source": [
    "<a id=\"deepdream\"></a>\n",
    "## DeepDream\n",
    "\n",
    "Now let's reproduce the [DeepDream algorithm](https://github.com/google/deepdream/blob/master/dream.ipynb) with TensorFlow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": []
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1457967388369,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "qM2U_96hyUwN",
    "outputId": "3725acc2-51cc-4894-e726-87bfe5727342",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def render_deepdream(t_obj, img0=img_noise,\n",
    "                     iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n",
    "    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
    "    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
    "    # Build the Laplacian normalization graph\n",
    "    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=4))\n",
    "\n",
    "    # split the image into a number of octaves\n",
    "    img = img0\n",
    "    octaves = []\n",
    "    for i in xrange(octave_n-1):\n",
    "        hw = img.shape[:2]\n",
    "        lo = resize(img, np.int32(np.float32(hw)/octave_scale))\n",
    "        hi = img-resize(lo, hw)\n",
    "        img = lo\n",
    "        octaves.append(hi)\n",
    "    \n",
    "    # generate details octave by octave\n",
    "    for octave in xrange(octave_n):\n",
    "        if octave>0:\n",
    "            hi = octaves[-octave]\n",
    "            img = resize(img, hi.shape[:2])+hi\n",
    "        for i in xrange(iter_n):\n",
    "            g = calc_grad_tiled(img, t_grad)\n",
    "            g = lap_norm_func(g)\n",
    "            img += g*(step / (np.abs(g).mean()+1e-7))\n",
    "            print '.',\n",
    "        clear_output()\n",
    "        showarray(img/255.0)\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some image and populate it with DogSlugs (in case you've missed them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1457967452116,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "M9_vOh_2Qgl-",
    "outputId": "eef01469-fb9b-4242-f249-e81383bf0433",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img0 = PIL.Image.open('testimages/mountains.jpg')\n",
    "img0 = np.float32(img0)\n",
    "showarray(img0/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Btw., this picture apparently contains a headland. Probably some sort of valley, alp or cliff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction('testimages/mountains.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1457967471615,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "k0oggbGEeC3U",
    "outputId": "c7258412-9cb1-4a94-d4f5-e6120a728c85",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = render_deepdream(tf.square(T('mixed4c')), img0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJzvhEFxpB7E"
   },
   "source": [
    "Recall how the first parameter to `render_*` was the optimization objecting. In the above case, we are optimizing for `square(mixed4c)`, or in english: \"Make an image in which layer `mixed4c` recognizes a lot of stuff. We're not optimizing for a specific feature any more, the network is just trying to overinterpret the image best it can.\n",
    "\n",
    "The network seems to like dogs and animal-like features due to the nature of the ImageNet dataset.\n",
    "\n",
    "However, we can force it to dream about a specific topic by changing the objective function to a specific feature or a combination of specific features. More castles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1457967665541,
     "user": {
      "color": "#1FA15D",
      "displayName": "Alexander Mordvintsev",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "12341152118244997759",
      "photoUrl": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",
      "sessionId": "1269ead540f76ce5",
      "userId": "108092561333339272254"
     },
     "user_tz": -60
    },
    "id": "4GexZuwJdDmu",
    "outputId": "f140b073-7129-4889-f240-3d0e00530ada",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = render_deepdream(T(layer)[:,:,:,65], img0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYsY6_Ngpfwl"
   },
   "source": [
    "Now, have fun! Upload your own images, and make the computer dream about the prettiest, creepiest, or most surreal things.\n",
    "\n",
    "Don't hesitate to use higher resolution inputs (also increase the number of octaves)! Here is an [example](http://storage.googleapis.com/deepdream/pilatus_flowers.jpg) of running the flower dream over a bigger image.\n",
    "\n",
    "The DeepDream [notebook](https://github.com/google/deepdream/blob/master/dream.ipynb) contains code with many more options to explore. You can guide the dreaming towards a specific image, or repeat it endlessly to produce dreamier dreams. If you're very patient, you can even make videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame = img0\n",
    "h, w = frame.shape[:2]\n",
    "s = 0.05 # scale coefficient\n",
    "for i in xrange(100):\n",
    "    frame = render_deepdream(tf.square(T('mixed4c')), img0=frame)\n",
    "    img = PIL.Image.fromarray(np.uint8(np.clip(frame, 0, 255)))\n",
    "    img.save(\"dream-%04d.jpg\"%i)\n",
    "    # Zoom in while maintaining size\n",
    "    img = img.resize(np.int32([w*(1+s), h*(1+s)]))\n",
    "    t, l = np.int32([h*(1+s) * s / 2, w*(1+s) * s / 2])\n",
    "    img = img.crop([l, t, w-l, h-t])\n",
    "    img.load()\n",
    "    print img.size\n",
    "    frame = np.float32(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mENNVQd3eD-h"
   },
   "source": [
    "<a id=\"fun\"></a>\n",
    "## More Fun\n",
    "\n",
    "For more more things to do, check out the [TensorFlow tutorials](http://tensorflow.org/tutorials). If you enjoyed this tutorial, you will probably like the [retraining example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/image_retraining).\n",
    "\n",
    "In this tutorial, we used the Inception v3, trained on imagenet. We have recently released the [source code to Inception](https://github.com/tensorflow/models/tree/master/inception), allowing you to train an Inception network on your own data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "deepdream2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
